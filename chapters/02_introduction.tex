\chapter{Introduction}

In a modern desktop computer equipped with a GPGPU, a single-threaded application cannot easily make use of more than 0.05 \% of the available performance. One hence has to learn the dark art of parallel programming to make a full use of the available performance.  

Indeed, consider an AMD Threadripper 3990X with NVIDIA GeForce RTX 4090 Ti. There, a single threaded application can utilise circa 49 GFLOPS, which is less than 0.05 \% of the overall performance, when one runs a single-threaded application once.
A multi-threaded application can utilise perhaps 3732 GFLOPS. A multi-threaded application not making use of the GPGPU can hence make use of less than 4 \%  of the overall performance. 
In contrast, a multi-threaded application can make use of almost 100 TFLOPS.
(To unlock this performance, one needs to use recent versions of OpenMP, SYCL, or vendor- or application-specific interfaces such as CUDA, OpenGL, or DirectX Compute. We will focus on OpenMP and SYCL in subsequent Chapters.)
Similar breakdown is available in a modern mobile phone (with a substantial mobile GPGPU, such ARM Mali-G710 MP7 in Google Tensor G2) and modern supercomputers.

This is related to the changing ``shape'' of computers: Computers no longer get faster, just ``wider'' and ``heterogeneous''. Data-parallel computing is most scalable approach to the ``wider'' computers.
  
In these lecture notes, you will learn about parallel computing and data-parallel computing, in particular. 
In the first three chapters, this we will learn to design multi-threaded applications. First with C++23, and subequently with OpenMP (``just in case''. In the following chapters, we will extend this to GPGPU-enabled systems and SYCL, and illustrate in a number of application domains.  

