\chapter{The Concepts}
\label{ch:concepts}

Parallelism means two or more tasks can be executed simultaneously. This is an option, which the compiler, operating system and processor can exercise, but does not come with any guarantees. 
Often, this means no shared variables or other resources, and need not require any synchronization primitives.

Concurrency means that two or more tasks start, run, and complete in overlapping time periods, while sharing some resources. 
If two tasks concurrently set a shared variable x to 1 and 2, it is not clear what value it would have, subsequently.
More broadly, concurrent access to a mutable shared memory can result in issues without the use of synchronization 
primitives (``data race'') and with the use of synchronization primitives (``deadlock'').

\subsection{Data race}

When we need to ensure mutual exclusion in access to two or more shared variables, 
e.g., read value of one of the variables and add it to another variable, 
we may need to use some synchronization primitives (e.g., mutexes).
Without the use of synchronization primitives, we are facing the risk of a data race. 

For example, consider the situation in a bank, where there are two clients:
Alice and Bob. 
Transaction T1: Bob has \$100 in his account, but will be paying a \$50 bill. 
At the same time, in Transaction T2, Alice will be paying \$100 to Bob.
Depending on the ordering of the reading and writing operations, one may obtain several outcomes:
\begin{itemize}
\item Transaction T1 will read \$100 valued of Bob's account. Transaction T2 will read \$100 value.
 Transaction T1 will write \$200. Transaction T2 will write \$50 value.
\item Transaction T1 will read \$100 valued of Bob's account. 
 Transaction T1 will write \$50. Transaction T2 will read \$50 value.Transaction T2 will write \$150 value.
\item Transaction T1 will read \$100 valued of Bob's account. Transaction T2 will read \$100 value.
 Transaction T1 will write \$50. Transaction T2 will write \$200 value.
\item Transaction T2 will read \$100 value. Transaction T2 will write \$200 value.
  Transaction T1 will read \$200 valued of Bob's account.  Transaction T1 will write \$150.  
\end{itemize}
Either Bob or the bank could be \$100 short. 

\subsection{Deadlock}

When we need to ensure mutual exclusion for access to two or more shared variables, 
e.g., two temporary results associated with two mutexes, 
one may naively lock the first mutex first, and subsequently lock the other mutex. This, however, 
can lead to a deadlock. One needs to lock both mutexes at the same time.  

Naively, one could run:

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/s7Yj9M5vf}{\ExternalLink}}
\footnotesize Locking multiple mutexes at once.
\tcblower
\cppfile{code_examples/cpp23/lock.h}
\end{codebox}

In theory, a deadlock (Czech: ``problém uváznutí'') can occur when:
\begin{itemize}
\item each lock is owned by one thread
\item each thread has locked at least one lock and needs to lock at least one more lock
\item it is impossible to remove the lock ownership
\item there is a cyclic dependency among the lock-using threads.
\end{itemize}

\subsection{Concurrent programming}

There are two essential models for concurrent programming: shared memory and message passing. In sharing memory, we have broadly four options:
\begin{itemize}
\item Confinement: Do not share memory between threads. This is often impossible.
\item Immutability: Do not share any mutable data between threads. 
\item Thread-safe code: Use data types with additional guarantees for storing any mutable data shared between threads, or even better, use implementations of algorithms that are already parallelized and handle the concurrency issues for you. For example in C++, one can use the standard template library (STL) with a suitable execution policy. In particular, the header \cpp{execution} defines objects \cpp{std::execution::seq}, \cpp{std::execution::par}, \cpp{std::execution::par_unseq}, which can be passed as the first argument of any standard algorithm, e.g., \\
\cpp{std::vector<int> my_data; std::sort(std::execution::par,my_data.begin(),my_data.end());}\\
See Section \ref{sec:foreachcpp23} for more examples.
\item Synchronization: Use synchronization primitives to prevent accessing the variable at the same time. This option is explored in this chapter in more detail. 
\end{itemize}
Eventually, we will see that message passing can be implemented using the synchronization primitives and may be the least challenging to use correctly. 

\section{Structuring code: Processes, Threads, Tasks, Coroutines}

Processes, threads, tasks, and coroutines execute instructions. 

A \emph{process} provides all of the prerequisites for executing instructions: loads an executable program, sets up a virtual address space, the environment (e.g. environment variables and a security context), the process control block (PCB, often stored in registers of the processor and on a per-process stack in kernel memory), opens handles to system objects (e.g., files, sockets), and often much more. In some sense, one can imagine ``a virtual machine''.

Within a particular process, there is at least one \emph{thread}. All threads of a particular process share the same virtual address space and handles to system objects. Each thread, independently, operates its own context (registers, stack, exception handlers). Unless declared otherwise, threads of a particular process share memory and are allocated ``time slices'' by the operating system. This can be seen as a ``virtual processor'' within a ``virtual machine'' of a process, often with no guarantees on the time slicing.

All modern operating systems (OS) are multitasking, i.e., running multiple processes with the operating system forcibly interrupting the run one process to execute another process after a certain amount of time (``preemptive scheduling''). Switching between the processes involves swapping the process control block (PCB). In Intel architectures, this is known as the task state segment (TSS), and there is hardware support for the switch. AMD64 does not support task switches in hardware.
Consequently, neither Windows nor Linux kernels utilize the hardware support for the switch.
Context switching thus has non-trivial impact on performance. 

Most modern processors are multi-core and support multithreading in some form. This means that single process can execute multiple ``hardware threads'', with support for switching between those. In Intel architectures, hyper-threading means each hardware core can execute multiple threads, e.g., two, to take advantage of idle time (e.g., loading data, network communications).  

Most modern operating systems \emph{do not guarantee} fairness among the threads. 

Within a particular thread, one may utilize multiple \emph{coroutines}, which can be seen as subroutines that can run in multiple steps, but sometimes can serve as a light-weight alternative to hardware threads. Coroutines can be called, can return when completed, but also can suspend themselves, yielding control and partial results, and be resumed by another co-routine. Typical uses involve generators andfactories and various other concepts within ``lazy evaluation'', as well as event-driven architectures within cooperative multitasking. 

That is: two coroutines within one thread never run in parallel, but one can have the runs of two or more coroutines interleaved. We can suspend a co-routine in one thread and resume it within another thread. 

As it turns out, the ``context switch'' with user-level threads has a similar cost to a function call or suspending a coroutine (\cpp{co_yield}). Indeed, coroutines are typically implemented with user-level threads, which leads to cheaper context-switch compared with hardware threads. Within the user-level threads, one can distinguish stackful and stackless versions, where coroutine state is saved on the heap (as in C++). 

A \emph{task} is a rather abstract unit of work, e.g., a function, which can be executed by any thread, but often allocated to one of a many threads within a pool. 


\section{Computational Complexity}

A large part of theoretical computer science is devoted to the study of the limitations of parallelizability of algorithms, starting with the very clear question: does every problem 
with a polynomial-time sequential algorithm also have an efficient parallel algorithm? In short, the answer is no. 

Let us consider the P-Complete problems \cite{greenlaw1995limits}, i.e.,  decision problems that are in P and where every problem in P can be reduced to the P-Complete problem in logarithmic space (L). The prime examples of P-Complete problems are 
\begin{itemize} 
\item circuit evaluation: given a Boolean circuit and an input, decide what the circuit outputs
\item linear programming: given a linear function subject to linear inequality constraints, find the optimum of the linear function
\item many graph problems, such as Lexicographically First Depth-first Search Ordering (LFDFS):
given an undirected graph with fixed ordered adjacency lists and two designated vertices $u$ and $v$, is vertex $u$ visited before vertex $v$ in the depth-first search of the graph?
\item many compression algorithms: given strings $s$ and $t$, will compressing $s$ with LZ78 add $t$ to the dictionary?
\item many problems related to Markov decision processes: is the minimum expected cost over all policies equal to 0 (for both finite, and long-run versions). 
\item many tests of local optimality in combinatorial optimization: in an instance of Traveling Salesman and a sequence of tours, is this a 2-Opt sequence? 
\end{itemize} 
Therein, we cannot hope for efficient solution on a parallel computer in the sense of solving in time $O(log^c n)$ using $O(n^k)$ parallel processors for some constants $c$ and $k$. 

\label{sec:Nicks}
In contrast, Nick's class NC$^{c}$ are decision problems solvable by a uniform family of Boolean circuits with polynomial size, depth $O(log^c(n))$, and fan-in 2. More usefully, a problem in NC with input of length $n$ can be solved in time $O(log^c n)$ using $O(n^k)$ parallel processors for some constants $c$ and $k$. (Notice that for a constant $k$, $O(n^k)$ is a polynomial.) Thus, NC can be thought of as problems that can be efficiently solved on a parallel computer. Integer arithmetics (addition, multiplication and division), matrix arithmetics (multiplication, determinant, inverse, rank), or multiple graph problems (shortest path, maximal matching with some restrictions on the weights) are in Nick's class and hence ``easier'' than P-Complete problems. 

\section{Hardware support}

\subsection{Memory hierarchies}

Most modern computers utilize complicated cache hierarchies. One may have 32 KB of L1 cache and 256-512 KB of L2 cache on each core, with cca. 1 ns and 3-4 ns latencies, respectively. (This will get reported as 32 MB of L2 cache on a Threadripper 3990X.) One may have many megabytes of L3 cache shared between the cores in some non-uniform fashion, with non-uniform latency of 10-20 ns. Finally, one may access the memory in cca. 60 ns. Compare these times with the 0.27 ns cycle time of a processor clocked at 3.7 GHz. 

Crucially, the times above are the best-case performance. To understand the worst-case performance, one needs to understand cache snooping (with a write-invalidate protocol). Most modern cores monitor a stream of memory loads and stores across the whole shared-memory machine. If a transaction concerns store in a block cached on the core (processor), the core sets some ``invalid'' flag to the block in its cache. If the core wishes to read a block with the ``invalid'' flag, it will ask all other cores for a valid value. Obtaining the correct value takes time. 

As we will see in the distributed-memory part of the course, one can also maintain the coherence otherwise, which is known as a directory-based coherency mechanism. Either way, cache coherence defines the requirements on concurrent reads and writes to the same address. To some extent, it masks the presence of cache hierarchies in any modern computing system. 

\subsection{GPGPUs}
\label{sec:GPGPUs}

As we have mentioned in the introduction, the bulk of the computational performance of most modern computing system (from a mobile phone to a supercomputer) is in its GPGPUs. Let us briefly consider a particular example of the NVIDIA Ampere architecture of GeForce RTX 3080 or NVIDIA A100:
\begin{itemize}
\item There are seven Graphics Processing Clusters (GPCs), sharing up to 40 MB of L2 cache and up to 40 GB of high-speed HBM2 memory 
\item Within each GPC, there are 12 Streaming Multiprocessors (SMs), 
\item Within each SM, there are 128 cores working with single-precision floating-point (FP32) precision and two double-precision (FP64) units. There is also 128 KB of L1/Shared Memory, shared across the 128 cores. 
\item Each SM is partitioned into four processing blocks (or partitions), each with a few kilobytes of L0 instruction cache and one warp scheduler.
\end{itemize}
Altogether, A100 has $7 \cdot 12 \cdot 128 = 10752$ cores, but their usage is rather constrained. Each warp can have at most 32 threads and runs them in lock-step on one processing blocks. Each streaming multiprocessors can run at most 64 warps of 32 threadblocks (i.e., 2048 threads per SM). Further constraints are due to the register use: each thread can use at most 255 registers, but there are only 65536 32-bit registers for the SM (yielding a limit of 257 threads per SM, at full register utilization). Further constraints are due to the use of memory hierarchy (esp. the 128 KB of shared memory, shared across the 128 cores). 

%The code is translated from CUDA C or C++ to Parallel Thread Execution (PTX), an intermediate representation, and subsequently to a proprietary assembly language (SASS). 

Similar to the CPU, the GPU hence has a memory hierarchy:
\begin{itemize}
\item L1 cache with 33 cycle latency and shared memory with even lower latency, based on microbenchmarking \cite{9926299} 
\item L2 cache with up to 2080 GB/s read bandwidth (200 cycle latency), based on microbenchmarking \cite{9926299} 
\item on-board HBM2 memory with 1555 GB/sec bandwidth (290 cycle latency)
\item intra-board NVLink with 50 Gb/sec per signal pair bandwidth
\item access to RAM via PCI Express Gen 4 (PCIe Gen 4) at 31.5 GB/sec
\item optionally, intra-node communication at 200 Gbit/sec using InfiniBand.
\end{itemize}
The interaction of the GPGPU memory hierarchy and CPU memory hierarchy is non-trivial, but summarized by the suggestion to reduce the number and volume of transfers between the host and the GPGPU, even at the price of increasing the volume of computation substantially. (Compare the numbers above to M.2 PCIe Gen4 SSDs with 7 GB/sec bandwidth.)

\subsection{Memory order}

Thus, there are several options for implementing synchronization primitives, known as ``memory orders'' or memory consistency. They define allowed behaviours of loads and stores to different addresses.
All guarantee atomicity and modification-order consistency.

In \cpp{memory_order_relaxed}, no further guarantees are provided and specifically no order is imposed on concurrent memory accesses. This is also how weakly-ordered architectures (e.g. ARM, NVIDIA) operate, by default: if two threads access shared memory, the load in one thread does not have to read a value written by another thread very recently, unless they use synchronization primitives (e.g. a fence).

With \cpp{memory_order_release} and \cpp{memory_order_acquire} specifiers, we force  weakly-ordered achitectures to behave closer to strongly-ordered architectures (e.g., Intel). If one thread writes into shared memory atomically with \cpp{memory_order_release} and another thread reads the memory atomically with \cpp{memory_order_acquire}, the load in the second thread is guaranteed to read the value written by another thread. 

With \cpp{memory_order_seq_cst} (sequential consistency), we additionally require a single total ordering of all modifications (with this specifier). A load with this specifier gets its value either from the last store with this specifier or from some store without this specifier that did not precede the most recent \cpp{memory_order_seq_cst store}. This is the default option and the strongest form of consistency, but it may also seem weak, in that one considers the ordering of the instructions the processor chose, rather than the ordering the programmer wrote down. 

(In earlier version of C++ standard, there were further memory models defined for the sake of DEC Alpha architecture. At least \cpp{memory_order_consume} is deprecated as of C++17.) 

See \cite{Poter2018} for an informal overview, \cite{Alglave2012} for a formal overview, and \url{https://www.youtube.com/watch?v=A_vAG6LIHwQ&ab_channel=ACCUConference} for a nice lecture. 

\subsection{Compare and swap}

Synchronization primitives are typically implemented using some hardware instructions, typically ``compare-and-swap''. In locking, these make it possible to test whether the lock is free, and if so, acquire the lock within a single operation that the hardware guarantees to execute atomically.

The atomic „compare and swap“ (CAS) instruction dereferences a pointer to an atomic variable and compares its value against a given value. If these is a match, it replaces the atomic variable with a given new value. That is:
\begin{itemize}
\item we declare an atomic variable (and a pointer to it)
\item (*) we save the value of an atomic variable to a local, private variable (by dereferencing the pointer)
\item based on the saved value in a local, private variable, we compute the new value, which we would like to store in the atomic variable
\item the CAS instruction is used. If the current value matches the value saved in the local, private variable, we will overwrite the value with the newly computed value. If the current value no longer matches the value saved in the local, private variable, we wait (some random and growing from a small starting value) and repeat from (*). 
\end{itemize}

In C++, the \cpp{atomic} header defines two variants of ``compare and swap'' and a specialization thereof for pointers:

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/jGb1PxoxY}{\ExternalLink}}
\footnotesize Definitions of two variants of ``compare and swap''.
\tcblower
\cppfile{code_examples/cpp23/atomic3.h}
\end{codebox}

The former is called with the desired value \cpp{i}, the new value {i}, and the memory order to consider if there is a match and if there is no match. Typically, if there is a match and we want to replace the value, we may use \cpp{std::memory_order_release}. If there is no match, we are just reading the value and \cpp{std::memory_order_acquire} would suffice. In the latter variant, we pass two pointers. 

The difference between the ``weak'' and ``strong'' variant is in that the weak variant may return false even if there is a match, in certain cases, but can be much faster on some architectures. This notably entails ARM architectures (RISC-V and MIPS), where the weak variant will be implemented using the so called load-link/store-conditional pair of instructions (load exclusive register / ldxr and store exclusive register / stxr in ARM version 8).
These are much faster than the comparable instructions issuing a barrier (ldaxr/stlxr in ARM version 8).
All four ARM instructions utilize only two registers, compared to three registers for CAS proper in Intel architectures (Compare and exchange / cmpxchg since 80486 and cmpxchg8b and cmpxchg16b since Intel Core 2). On recent Intel and AMD processors, cmpxchg is only marginally slower than a non-cached load.

In C++, the \emph{only} synchronization primitive that is guaranteed to be hardware implemented is a particular atomic boolean type, which is known as \cpp{std::atomic_flag}. Unlike all specializations of \cpp{std::atomic}, it is guaranteed to be lock-free. Prior to C++20, it has been very restricted, because there was no way to check the value of \cpp{std::atomic_flag} without setting it. C++20 adds method \cpp{test()}. 

%\subsection{Fetch and add}

% GPGPU hardware: stream multiprocessor (SM)

\section{Synchronization primitives}

Synchronization primitives make it possible to synchronize or restrict access of multiple threads to some resources (e.g., global variables, file handles, sockets). You can use them as an interface, without knowing their implementation. 

\subsection{Raw synchronization primitives}

Lock, Mutex, Semaphore, Atomic, Memory Fence, Condition Variable are synchronization primitives, which make it possible to synchronize or restrict access of multiple threads to some resources. 

Lock is a very general term for a synchronization primitive. Mutexes are usually used by one thread only, while semaphores are shared between multiple threads. The \emph{binary semaphore} is the most simple type of a lock, which provides exclusive access for both reading and writing. \emph{counting semaphore} limits the use of a single resource by at most a given number of threads. 

A spinlock, the thread simply waits ("spins") until the lock becomes available. This is efficient if threads are blocked for a short time, because it avoids the overhead of operating system process re-scheduling. It is inefficient if the lock is held for a long time, or if the progress of the thread that is holding the lock depends on preemption of the locked thread. An intentionally simplistic implementation is presented below.

\raggedbottom
\begin{codebox}[]{\href{https://godbolt.org/z/h1chd99aM}{\ExternalLink}}
\footnotesize A silly implementation of a spin lock.
\tcblower
\cppfile{code_examples/cpp23/atomic4.cpp}
\end{codebox}

\subsection{Further synchronization features}

Fences help order non-atomic and atomic memory accesses, without any associated operations. On Intel architectures (including x86-64), \cpp{atomic_thread_fence} do not issue any instructions, except for\\\cpp{std::atomic_thread_fence(std::memory_order::seq_cst)}.

Barrier provides a thread-coordination mechanism that blocks a group of threads until all threads in that group have reached the barrier. Such a barrier can be used repeatedly to wait until a number of threads have finished their operations. 

Latch and is a downward counter, whose initial value is initialized and then threads may block on the latch until the counter is  zero. One thread may decrement a latch multiple times, but no thread can increment the latch. Thus, it serves as a single-use barrier.

We will also see synchronized output streams. The synchronized buffer is flushed only when the destructor of the synchronized buffer is called, but provides for guarantees of atomicity for the access. (That is, \cpp{endl} and \cpp{std::flush} no longer flush!)
